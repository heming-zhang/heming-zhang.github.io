---
layout: notes
section-type: notes
title: 蒙特卡罗方法与马尔科夫链 MCMC
category: cn_misc
---

* TOC
{:toc}
---

本文部分内容摘自：
* [**LDA数学八卦**](http://bloglxm.oss-cn-beijing.aliyuncs.com/lda-LDA%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6.pdf)
* CNblogs [MCMC 蒙特卡洛方法](https://www.cnblogs.com/pinard/p/6625739.html)
* CNBlogs [MCMC 马尔科夫链](http://www.cnblogs.com/pinard/p/6632399.html)
* 参考视频 [mcmc与gibbs采样](https://v.youku.com/v_show/id_XMzMxNDM4Mzk4MA==.html)

马尔科夫链蒙特卡罗(Markov Chain Monte Carlo)，作为一种随机采样方法，在机器学习、深度学习以及自然语言处理等领域都有广泛的应用，是很多算法求解的基础，主要是因为MCMC可以用来做一些复杂运算的近似求解。

<br>
<br>
<br>

## 蒙特卡罗方法引入
<hr>

蒙特卡罗原来是一个赌场的名字，用它作为名字是因为蒙特卡罗方法是一种随机模拟方法，这很像赌场里面掷骰子的过程，最早的蒙特卡罗方法都是为了求解一些不太好求解的求和或者积分问题。比如积分:  

$$\theta = \int_a^b{f(x)dx} \tag{1}$$

如果我们很难求解出$f(x)$的原函数，那么这个积分就比较难求解。这里我们可以使用蒙特卡罗方法来模拟求解近似值。假设我们的函数图像如下图：

<center>
<img src=".\pictures\005.png">
Figure 1
</center>

那么，一个简单的近似求解方法就是在$[a,\ b]$之间随机的采样一个点。比如$x_0$，然后用$f(x_0)$代表在$[a,\ b]$区间上所有的$f(x)$的值。那么上面的定积分可以近似求解为： 

$$(b-a)f(x_0)$$

当然，用一个值代表$[a,\ b]$区间上所有的$f(x)$值，这个假设太粗糙。那么我们可以采样$[a,\ b]$区间的$n$个值：$x_0,\ x_1,\cdots,x_{n-1}$,用它们的均值来代表$[a,\ b]$区间上所有的$f(x)$值。这样我们上面的定积分可以近似求解为：

$$\frac{b-a}{n}\sum_{i=0}^{n-1}f(x_i) \tag{2}$$

然而，上面的方法可以一定程度上求解出近似的解，但是它隐含了，即$x$在$[a,\ b]$之间是均匀分布的，而绝大部分情况，$x$在$[a,\ b]$之间不是均匀分布的。如果我们用上面的方法，则模拟求出的结果很可能和真实值相差甚远。

这里我们通过如下的方式解决这个问题，我们可以得到$x$在$[a,\ b]$的概率分布函数$p(x)$，那么我们的定积分求和可以这样进行：

$$\theta=\int_a^bf(x)dx=\int_a^b\frac{f(x)}{p(x)}p(x)dx\approx\frac{1}{n}\sum_{i=0}^{n-1}\frac{f(x_i)}{p(x_i)} \tag{3}$$

上式最右边的这个形式就是蒙特卡罗方法的一般形式。当然这里是连续函数形式的蒙特卡罗方法，但是在离散时一样成立。可以看出，最上面我们假设$x$在$[a,\ b]$之间是均匀分布的时候，$p(x_i)=\frac{1}{b-a}$，代入$(3)$式最右边，可以得到：

$$\frac{1}{n}\sum_{i=0}^{n-1}\frac{f(x_i)}{\frac{1}{b-a}}=\frac{b-a}{n}\sum_{i=0}^{n-1}f(x_i)$$

也就是说，我们最上面的均匀分布也可以作为一般概率分布函数$p(x)$在均匀分布时候的特例。

<br>
<br>
<br>

## 概率分布采样
<hr>

上面我们讲到蒙特卡罗方法的关键是得到$x$的概率分布。如果求出了$x$的概率分布，我们可以基于概率分布去采样基于这个概率分布的$n$个$x$的样本集，代入蒙特卡罗求和的式子即可求解。但还有一个问题：如何基于概率分布去采样基于这个还绿分布的$n$个$x$的样本集。

对于常见的均匀分布$Uniform(0,\ 1)$是非常容易采样的，一般同股哟线性同余发生器可以很方便的生成$(0,\ 1)$之间的伪随机数样本，而其他的常见的概率分布也是可以通过$Uniform(0,\ 1)$的样本转换而得。比如二维正态分布的样本$(Z_1,\ Z_2)$可以通过独立采样得到的$Uniform(0,\ 1)$样本对$(X_1,\ X_2)$通过如下式子转换而得：

$$Z_1=\sqrt{-2lnX_1}cos(2\pi{X_2})$$
$$Z_2=\sqrt{-2lnX_1}sin(2\pi{X_2})$$

但在一些时候，概率分布函数不是常见的类型，这意味着我们没法方便的得到这些非常见的概率分布的样本集。那这个问题怎么解决呢？

<br>
<br>
<br>

## 接受-拒绝采样
<hr>

对于概率分布不是常见的分布，一个可行的办法是采用接受-拒绝采样来得到该分布的样本。既然$p(x)$太复杂在程序中没法直接采样，那么我设定一个程序可采样的分布$q(x)$比如高斯分布，然后按照一定的方法拒绝某些样本，以达到接近$p(x)$分布的目的，其中$q(x)$叫做 proposal distribution。


<center>
<img src=".\pictures\006.png">
Figure 1
</center>

具体采样过程如下，设定一个方便采样的常用概率分布函数$q(x)$，以及一个常量$k$，使得$p(x)$ 总在$kq(x)$的下方。如上图。

首先，采样得到$q(x)$的一个样本$z_0$，采样方法如第三节。然后，从均匀分布$(0,\ kq(z_0))$中采样得到一个值$u$。如果$u$落在了上图中的灰色区域，则拒绝这次抽样，否则接受这个样本$z_0$。重复以上过程得到n个接受的样本$z_0,\ z_1,\cdots,\ z_{n-1}$,则最后的蒙特卡罗方法求解结果为：

$$\frac{1}{n}\sum_{i=0}^{n-1}{\frac{f(z_i)}{p(z_i)}}\tag{4}$$

（其实我们发现$(3)$式$(4)$式的数学形式是一样的，只不过$(4)$式需要通过接受拒绝采样来进行评估。）

在整个过程中，是否接受该次取样值需要通过上述过程中的采样规则来决定，这样以来，我们就很容易得到我们所需要的概率值了。

<br>
<br>
<br>

## 马尔科夫链
<hr>

马尔科夫链用于表示状态的转换，从一种状态转移到另一种状态。例如，我们使用父代和子代之间的关系转换为例子：

| state | 子代1 | 子代2 | 子代3 | 
| :------: | :------: | :------: | :------: |
| 父代1 | 0.65 | 0.28 | 0.07 |
| 父代2 | 0.15 | 0.67 | 0.18 |
| 父代3 | 0.12 | 0.36 | 0.52 |

由此，我们可以得到一个状态转移矩阵：

$$
P=\left[
 \begin{matrix}
   0.65 & 0.28 & 0.07 \\
   0.15 & 0.67 & 0.18 \\
   0.12 & 0.36 & 0.52
  \end{matrix}
  \right] \tag{3}
$$

由此我们可以计算第$n+1$代中处于第$j$个阶层的概率：

$$\pi(X_{n+1=j})=\sum_{i=0}^{n}\pi(X_n=i)\cdot{P(x_{n+1}=j|x_n=i)}$$

这里我们给定一个初始概率，$P_i=(0.21,\ 0.68,\ 0.11)$，迭代结果如下：

<table>
<tr>
    <td>第n代</td>
    <td>第1阶层</td>
    <td>第2阶层</td>
    <td>第3阶层</td>
</tr>
<tr>
    <td>0</td>
    <td>0.21</td>
    <td>0.68</td>
    <td>0.11</td>
</tr>
<tr>
    <td>1</td>
    <td>0.252</td>
    <td>0.554</td>
    <td>0.194</td>
</tr>
    <td>2</td>
    <td>0.27</td>
    <td>0.512</td>
    <td>0.218</td>
</tr>
<tr>
    <td>3</td>
    <td>0.278</td>
    <td>0.497</td>
    <td>0.225</td>
</tr>
<tr>
    <td>4</td>
    <td>0.282</td>
    <td>0.49</td>
    <td>0.226</td>
</tr>
<tr>
    <td>5</td>
    <td>0.285</td>
    <td>0.489</td>
    <td>0.225</td>
</tr>
<tr>
    <td bgcolor="coral">6</td>
    <td bgcolor="coral">0.286</td>
    <td bgcolor="coral">0.489</td>
    <td bgcolor="coral">0.225</td>
</tr>
<tr>
    <td bgcolor="coral">7</td>
    <td bgcolor="coral">0.286</td>
    <td bgcolor="coral">0.489</td>
    <td bgcolor="coral">0.225</td>
</tr>
<tr>
    <td bgcolor="coral">8</td>
    <td bgcolor="coral">0.286</td>
    <td bgcolor="coral">0.489</td>
    <td bgcolor="coral">0.225</td>
</tr>
<tr>
    <td bgcolor="coral">9</td>
    <td bgcolor="coral">0.286</td>
    <td bgcolor="coral">0.489</td>
    <td bgcolor="coral">0.225</td>
</tr>
<tr>
    <td bgcolor="coral">10</td>
    <td bgcolor="coral">0.286</td>
    <td bgcolor="coral">0.489</td>
    <td bgcolor="coral">0.225</td>
</tr>
</table>

那么，在上述的规律中，第1代中第1阶层的概率应该是：

$$0.21\times0.65+0.68\times0.15+0.11\times0.12=0.252$$

而且，我们发现经过转移矩阵的几次计算之后，$n$代之后各个阶层状态的概率不再改变，这就是马尔科夫链的一个性质。

于是有**马尔科夫链定理**：如果一个非周期马尔科夫链具有转移概率矩阵$P$，且其任何两个状态是连通的，那么${\lim_{n\to+\infty}P_{ij}^n=\pi(j)}$，我们有：

$$\lim_{n\to+\infty}P^n=
\left[
 \begin{matrix}
   \pi(1) & \pi(2) & \cdots & \pi(j) & \cdots \\
   \pi(1) & \pi(2) & \cdots & \pi(j) & \cdots \\
   \cdots & \cdots & \cdots & \cdots & \cdots \\
   \pi(1) & \pi(2) & \cdots & \pi(j) & \cdots \\
   \cdots & \cdots & \cdots & \cdots & \cdots 
  \end{matrix}
  \right]$$  

$$\pi(j)=\sum_{i=0}^{\infty}\pi(i)P_{ij}$$  

$\pi$是方程$\pi{P}=\pi$的唯一非负解

其中，

$$\pi=[\pi(1),\ \pi(2),\cdots,\ \pi(j),\cdots],\ \sum_{i=0}^{\infty}\pi_i=1$$

$\pi$称为马尔科夫链中的平稳分布。

由此，我们可以引申出马尔科夫**细致平稳条件**：  
如果非周期马尔科夫链的转移矩阵$P$和分布$\pi(x)$满足

$$\pi(i)P_{ij}=\pi(j)P_{ji}\quad\forall i,j\in\mathbb N$$ 

则$\pi(x)$是马尔科夫链的平稳分布，上式被称为细致平稳分布条件（detailed balance condition）。

<br>
<br>
<br>

## Metropolis采样
<hr>

* 接着上面的细致平稳分布，我们希望通过一个转移矩阵最终得到所期望的目标函数$p(x)$的分布。这里，假设我们已经得到一个转移矩阵$Q$。  
* 其中马尔科夫链$q(i,j)$表示从状态$i$转移到状态$j$的概率，也可以写作$q(j\|i)$或者$q(i\rightarrow{j)}$。

那么，显然通常情况下：

$$p(i)q(i,j)\neq{p(j)q(j,i)} \tag{4}$$
也就是细致平稳条件不成立，所以$p(x)$不太可能是这个马尔科夫链的平稳分布。但这里，我们可以对$(4)$式进行一定的调整，从而使它满足细致平稳条件，譬如我们可以引入一个$\alpha(i,j)$，有：

$$p(i)q(i,j)\alpha(i,j)=p(j)q(j,i)\alpha(j,i)\tag{5}$$
那么，取什么样的$\alpha(i,j)$和$\alpha(j,i)$可以使上式成立呢？最简单的，按照对称性，我们可以取

$$\alpha(i,j)=p(j)q(j,i)\quad\ \alpha(j,i)=p(i)q(i,j) \tag{5*}$$
于是$(5)$式就成立了。所以有

$$p(i)\underbrace{q(i,j)\alpha(i,j)}_{Q'(i,j)}=p(j)\underbrace{q(j,i)\alpha(j,i)}_{Q'(j,i)}\tag{6}$$

于是，我们把原来具有转移矩阵$Q$的马尔科夫链改造成了一个具有转移矩阵$Q'$的马尔科夫链，而$Q'$恰好满足细致平稳条件，由此马尔科夫链$Q'$的细致平稳分布就是$p(x)$！

总结一下，上面我们通过引入接受率$\alpha(i,j)$，这里的接受率就是在之前的蒙特卡罗方法中是否接受采样的依据，那么下图就可以很好地表示这个过程：

<center>
<img src=".\pictures\007.png">
Figure 2
</center>

那么下面我们就将上述过程投入到采样的应用中去，其中首先投入应用的是Metropolis Sampling为例考虑，Metropolis是一位物理学家，他发现了蒙特卡罗模拟法可以用来计算能量分布的函数，后来这种方法被渐渐的应用在了统计领域。
 
Metropolis算法具体思想如下；

| Algotithm1 MCMC 采样算法 | 
| :------ |
|1：初始化马尔科夫链初始状态$X_0=x_0$ |
|2: 对$t=0,1,2,\cdots,$循环以下过程进行采样 |
|&nbsp;&nbsp;(1)第$t$个时刻马尔科夫链状态为$X_t=x_t$，采样${y}\sim{q(x_t, x)}$ |
|&nbsp;&nbsp;(2)从均匀分布采样采样${u}\sim{Uniform[0,1]}$ |
|&nbsp;&nbsp;(3)如果$u<\alpha(x_t,y)=p(y)q(y,x_t)$,则接受转移$x_t\rightarrow{y},$即$X_{t+1}=y$ |
|&nbsp;&nbsp;(4)否则不接受转移，即$X_{t+1}=x_t$ |


说明：  
* 上述算法中$p(x),\ q(y,x)$说的都是离散的情形，事实上几遍这两个分布是连续的，以上算法仍然有效，于是我们可以得到更一般的连续概率分布$p(x)$的采样算法。
* 上述算法在执行循环的第三步时，需要进行判断。这里的$\alpha(x_t,y)=p(y)q(y,x_t)$，其实是根据$(5)和(5*)$式推算出来的
* 上述算法中$q(y,x)$中的$y$其实是一个过渡的采样分布，如果接受了$y$，那么我们的采样就会得到更新，否则就不更新。

以上的MCMC采样算法已经能很漂亮的工作了，不过它有一个小的问题：马尔科夫链$Q$在转移的过程中的接受率$\alpha(i,j)$（也就是上述算法中的$\alpha(x_t,y)$）可能偏小，这样采样过程中马尔科夫链会很容易原地踏步，拒绝大量的跳转，这使得马尔科夫链遍历所有的状态空间要花费太长的时间，收敛到平稳分布$p(x)$的速度太慢。有没有办法进一步提升一些接受率呢？

假设$\alpha(i,j)=0.1, \alpha(j,i)=0.2$， 此时满足细致平稳条件，于是有：

$$p(i)q(i,j)\times0.1=p(j)q(j,i)\times0.2$$

将上面式子的两边扩大5倍，我们有：

$$p(i)q(i,j)\times0.5=p(j)q(j,i)\times1$$

看到了没有！我们提高了接受率，而细致平稳条件并没有被打破！这启发我们可以把$(6)$式中的$\alpha(i,j),\ \alpha(j,i)$同比例放大，使得两个数中最大的一个放大到1，这样我们就提高了采样中的跳转接受率。所以我们可以取：

$$\alpha(i,j)=min\lbrace{\frac{p(j)q(j,i)}{p(i)q(i,j)},1}\rbrace$$

于是，经过对上述MCMC采样算法中接受率的微小改造，我们就得到了如下教科书中最常见的**Metropolis-Hastings算法**。

对于分布$p(x)$，我们构造转移矩阵$Q'$使其满足细致平稳条件

$$p(x)Q'(x\rightarrow{y})=p(y)Q'(y\rightarrow{x})$$

此处$x$并不要求是一维的，对于高维空间的$p({\bf x})$，如果满足细致平稳条件

$$p({\bf x})Q'({\bf x}\rightarrow{\bf y})=p({\bf y})Q'({\bf y}\rightarrow{\bf x})$$

于是我们可以得到一个新的算法：
| Algotithm2 Metropolis-Hastings 采样算法 | 
| :------ |
|1：初始化马尔科夫链初始状态$X_0=x_0$ |
|2: 对$t=0,1,2,\cdots,$循环以下过程进行采样 |
|&nbsp;&nbsp;(1)第$t$个时刻马尔科夫链状态为$X_t=x_t$，采样${y}\sim{q(x_t, x)}$ |
|&nbsp;&nbsp;(2)从均匀分布采样采样${u}\sim{Uniform[0,1]}$ |
|&nbsp;&nbsp;(3)如果$u<\alpha(i,j)=min\lbrace{\frac{p(j)q(j,i)}{p(i)q(i,j)},1}\rbrace$,则接受转移$x_t\rightarrow{y},$即$X_{t+1}=y$ |
|&nbsp;&nbsp;(4)否则不接受转移，即$X_{t+1}=x_t$ |

<br>
<br>
<br>

## Gibbs 采样
<hr>

对于高纬的情形，由于接受率$\alpha$的存在（通常$\alpha<1$），以上Metropolis-Hastings算法的效率不够高。能否找到一个转移矩阵$Q$使得接受率$\alpha=1$呢？我们先看看二维的情况，假设有一个概率分布$p(x,y)$，考察$x$坐标相同的两个点$A(x_1,y_1),B(x_1,y_2)$，我们发现  

$$p(x_1,y_1)p(y_2|x_1)=p(x_1)p(y_1|x_1)p(y_2|x_1)$$  

$$p(x_1,y_2)p(y_1|x_1)=p(x_1)p(y_2|x_1)p(y_1|x_1)$$

所以得到

$$p(x_1,y_1)p(y_2|x_1)=p(x_1,y_2)p(y_1|x_1) \tag{7}$$
即

$$p(A)p(y_2|x_1)=p(B)p(y_1|x_1)$$

基于以上公式，我们发现，在$x=x_1$这条平行与$y$轴的直线上，如果使用条件分布$p(y\|x_1)$作为任何两个点之间的转移概率，那么任何两个点之间的转移满足细致平稳条件。同样的，如果我们在$y=y_1$这条直线上任意取两个点$A(x_1,y_1),C(x_2,y_1)$，也有如下等式  

$$p(A)p(x_2|y_1)=p(C)p(x_1|y_1)$$

上述的变化如下图所示：
<center>
<img src=".\pictures\008.png">
Figure 3
</center>

于是，我们可以构造转移矩阵$Q$

$$
\begin{aligned}
& Q(A\rightarrow{B})=p(y_B|x_1)\quad & 如果\ x_A=x_B=x_1\\
& Q(A\rightarrow{C})=p(x_C|x_1)\quad & 如果\ y_A=y_C=y_1\\
& Q(A\rightarrow{D})=0\quad & 其它
\end{aligned}
$$

有了如上的转移矩阵$Q$，我们很容易验证：对平面上任意两点$X,Y$，满足细致平稳条件

$$p(X)Q(X\rightarrow{Y})=p(Y)Q(Y\rightarrow{X})$$

于是这个二位空间上的马尔科夫链将收敛到平稳分布$p(x,y)$。而这个算法就称为Gibbs Sampling算法，是由物理学家Gibbs首先给出的。

| Algotithm3 二维Gibbs Sampling 算法 | 
| :------ |
|1：随机初始化$X_0=x_0,\ Y_0=y_0$ |
|2: 对$t=0,1,2,\cdots,$循环采样 |
|&nbsp;&nbsp;(1)$\ {y_{t+1}}\sim{p(x_t, y)}$ |
|&nbsp;&nbsp;(2)$\ {x_{t+1}}\sim{p(y_{t+1}, x)}$ |

以上采样过程中，如图4所示，马尔科夫链的转移知识轮换的沿着坐标轴$x$轴和$y$轴做转移，于是得到样本$(x_0,y_0),(x_0,y_1),(x_1,y_1),(x_1,y_2),(x_2,y_2),\cdots$
<center>
<img src=".\pictures\009.png">
Figure 4
</center>

马尔科夫链收敛后，最终得到的样本就是$p(x,y)$的样本，而收敛之前的阶段称为burn-in period。额外说明一下，我们看到教科书上的Gibbs Sampling大都是坐标轴轮换采样的，但是这其实是不强制要求的。最一般的情形可以是，在$t$时刻，可以在$x$轴和$y$轴之间随机的选一个坐标轴，然后按条件概率做转移，马尔科夫链也是一样收敛的。轮换两个坐标轴只是一种方便的形式。

上述的过程很容易推广到高维的情形，对$(7)$式，如果$x_1$变为多维情形${\bf {x_1}}$，可以看出推导过程不变，所以细致平稳条件同样是成立的

$$p({\bf {x_1}},y_1)p(y_2|{\bf {x_1}})=p({\bf {x_1}},y_2)p(y_1|{\bf {x_1}}) \tag{8}$$

此时，转移矩阵$Q$由条件分布$p(y\|{\bf {x_1}})$定义。上式只是说明一个坐标轴的情况和二维情形类似，很容易验证对所有坐标轴都有类似的结论。所以$n$维空间中对于概率分布$p(x_1,x_2,\cdots,x_n)$可以如下定义转移矩阵

* 如果当前状态为$p(x_1,x_2,\cdots,x_n)$，马尔科夫链转移的过程中，只能沿着坐标轴做转移。沿着$x_i$这个坐标轴做转移的时候，转移概率由条件概率$p(x_i\|x_1,\cdots,x_{i-1},x_{i+1},\cdots,x_n)$定义；

* 其它无法沿着单个坐标轴进行的跳转，转移概率都设置为0

于是我们可以把Gibbs Sampling算法从采样二维的$p(x,y)$推广到采样$n$维的$p(x_1,x_2,\cdots,x_n)$

| Algotithm3 二维Gibbs Sampling 算法 | 
| :------ |
|1：随机初始化$X_0=x_0,\ Y_0=y_0$ |
|2: 对$t=0,1,2,\cdots,$循环采样 |
|&nbsp;&nbsp;(1)$\ {x_1^{(t+1)}}\sim{p((x_2^{(t)},x_3^{(t)},\cdots,x_n^{(t)}), x_1)}$ |
|&nbsp;&nbsp;(2)$\ {x_2^{(t+1)}}\sim{p((x_1^{(t+1)},x_3^{(t)},\cdots,x_n^{(t)}), x_2)}$ |
|&nbsp;&nbsp;(3)$\ \cdots$ |
|&nbsp;&nbsp;(4)$\ {x_j^{(t+1)}}\sim{p((x_1^{(t)},\cdots,\ x_{j-1}^{(t+1)},x_{j+1}^{(t)},\cdots,x_n^{(t)}), x_j)}$ |
|&nbsp;&nbsp;(5)$\ \cdots$ |
|&nbsp;&nbsp;(6)$\ {x_1^{(t+1)}}\sim{p((x_1^{(t+1)},x_2^{(t+1)},\cdots,x_{n-1}^{(t)}), x_n)}$ |


以上算法收敛后，得到的就是概率分布$p(x_1,x_2,\cdots,x_n)$的样本，当然这些样本并不独立。同样的，在以上的算法中，坐标轴轮换采样不是必须的，可以在坐标轴轮换中引入随机性，这时候转移矩阵$Q$中任何两个点的转移概率中就会包含坐标轴选择的概率，而在通常的Gibbs Sampling算法中，坐标轴轮换是一个确定性的过程，也就是在给定时刻$t$，在一个固定的坐标轴上转移的概率是1。